# 146. LRU Cache
## 146. LRU 缓存
>Design a data structure that follows the constraints of a Least Recently Used (LRU) cache.  
Implement the LRUCache class:  
LRUCache(int capacity) Initialize the LRU cache with positive size capacity.  
int get(int key) Return the value of the key if the key exists, otherwise return -1.   
void put(int key, int value) Update the value of the key if the key exists. Otherwise, add the key-value pair to the cache. If the number of keys exceeds the capacity from this operation, evict the least recently used key.  
The functions get and put must each run in O(1) average time complexity.  

设计一个数据结构，最近最少使用内存。   

>Example 1:  
Input  
["LRUCache", "put", "put", "get", "put", "get", "put", "get", "get", "get"]  
[[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]  
Output  
[null, null, null, 1, null, -1, null, -1, 3, 4]  

Explanation  
LRUCache lRUCache = new LRUCache(2);  
lRUCache.put(1, 1); // cache is {1=1}  
lRUCache.put(2, 2); // cache is {1=1, 2=2}  
lRUCache.get(1);    // return 1  
lRUCache.put(3, 3); // LRU key was 2, evicts key 2, cache is {1=1, 3=3}  
lRUCache.get(2);    // returns -1 (not found)  
lRUCache.put(4, 4); // LRU key was 1, evicts key 1, cache is {4=4, 3=3}  
lRUCache.get(1);    // return -1 (not found)   
lRUCache.get(3);    // return 3   
lRUCache.get(4);    // return 4  
 

>Constraints:  
1 <= capacity <= 3000  
0 <= key <= 104  
0 <= value <= 105  
At most 2 * 105 calls will be made to get and put.  
## 思考
比较简单的方法就是直接利用一个数据结构存储最后一次调用时间，每次对调用时间进行比较确定是否需要对某个元素进行删除。  
但是这样就会比较消耗时间，无论执行什么操作都会对整个列表进行循环比较判断。 

不如： 换个方向  
从 Example 中可以看出，每次调用或者插入某个key这个key的权重都会增加。   
创建一个list每次前面一个存放权重最高的，后面的权重放最低的。  
每次删除的时候只需要删除最后一个就可以了。   
更新权重的时候需要对元素进行排序，会有可能需要将某个元素的位置删除放到第一个位置。  
第一次提交的时候忘记了，如果put的时候key已经存在需要更新。  

利用list调整key和优先级，dict存储键值对，每次用key进行判断。 

```python3
class LRUCache:

    def __init__(self, capacity: int):
        self.capacity = capacity
        self.dict = {}
        self.list = []
        self.real_c = 0
    def __flash__(self, key: int):
        self.list.remove(key)
        self.list.insert(0, key)

    def get(self, key: int) -> int:
        ret = 0
        if key in self.dict:
            ret = self.dict[key]
            self.__flash__(key)
            
        else:
            ret = -1
        return ret

    def put(self, key: int, value: int) -> None:
        if key in self.dict:
            self.dict[key] = value
            self.__flash__(key)
        else:
            if self.real_c < self.capacity:
                self.dict[key] = value
                self.list.insert(0, key)
                self.real_c += 1
            else:
                keytmp = self.list.pop()
                del self.dict[keytmp]

                self.dict[key] = value
                self.list.insert(0, key)

```
>Runtime: 4570 ms, faster than 5.00% of Python3 online submissions for LRU Cache.  
Memory Usage: 75.8 MB, less than 47.26% of Python3 online submissions for LRU Cache.

哈哈哈 这个结果我敢说也是没谁了。时间也太久了，内存也不高效。  
## 答案
### 哈希表+双向链表
双向链表按照被使用的顺序存储了这些键值对，靠近头部的键值对是最近使用的，而靠近尾部的键值对是最久未使用的。  
哈希表即为普通的哈希映射（HashMap），通过缓存数据的键映射到其在双向链表中的位置。

基本上的思路和我一样，但是利用双向链表来实现了。速度上提升很多。  
用双向链表调整优先级和进行存储，用dict来存储key和链表节点。   
```python3
class DLinkedNode:
    def __init__(self, key=0, value=0):
        self.key = key
        self.value = value
        self.prev = None
        self.next = None


class LRUCache:

    def __init__(self, capacity: int):
        self.cache = dict()
        # 使用伪头部和伪尾部节点    
        self.head = DLinkedNode()
        self.tail = DLinkedNode()
        self.head.next = self.tail
        self.tail.prev = self.head
        self.capacity = capacity
        self.size = 0

    def get(self, key: int) -> int:
        if key not in self.cache:
            return -1
        # 如果 key 存在，先通过哈希表定位，再移到头部
        node = self.cache[key]
        self.moveToHead(node)
        return node.value

    def put(self, key: int, value: int) -> None:
        if key not in self.cache:
            # 如果 key 不存在，创建一个新的节点
            node = DLinkedNode(key, value)
            # 添加进哈希表
            self.cache[key] = node
            # 添加至双向链表的头部
            self.addToHead(node)
            self.size += 1
            if self.size > self.capacity:
                # 如果超出容量，删除双向链表的尾部节点
                removed = self.removeTail()
                # 删除哈希表中对应的项
                self.cache.pop(removed.key)
                self.size -= 1
        else:
            # 如果 key 存在，先通过哈希表定位，再修改 value，并移到头部
            node = self.cache[key]
            node.value = value
            self.moveToHead(node)
    
    def addToHead(self, node):
        node.prev = self.head
        node.next = self.head.next
        self.head.next.prev = node
        self.head.next = node
    
    def removeNode(self, node):
        node.prev.next = node.next
        node.next.prev = node.prev

    def moveToHead(self, node):
        self.removeNode(node)
        self.addToHead(node)

    def removeTail(self):
        node = self.tail.prev
        self.removeNode(node)
        return node
```
## 总结
[us](https://leetcode.com/problems/lru-cache/)
[cn](https://leetcode.cn/problems/lru-cache/?favorite=2cktkvj)